# ğŸ¤– Robustness of Transformer-Based Models in Noisy Sentiment Analysis  
## _Deep Learning Evaluation with BERT vs BiLSTM on Twitter Data_

## ğŸ” Overview

Transformer models like **BERT** are leading the NLP space â€” but how do they hold up under noisy, real-world inputs like Twitter typos or slang? This study evaluates the **robustness** of **BERT** and a **BiLSTM** model on **Twitter sentiment analysis**, using datasets from **SemEval 2015 & 2017**. We apply **linguistic noise**, test both models under clean and perturbed conditions, and provide a comparative performance report.

...

## ğŸ“š Citation

```text
Ibok, B. (2025). Robustness of Transformer-Based Models Against Linguistic Noise and Adversarial Inputs in Social Media Sentiment Tasks. Coventry University.
```