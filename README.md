# 🤖 Robustness of Transformer-Based Models in Noisy Sentiment Analysis  
## _Deep Learning Evaluation with BERT vs BiLSTM on Twitter Data_

## 🔍 Overview

Transformer models like **BERT** are leading the NLP space — but how do they hold up under noisy, real-world inputs like Twitter typos or slang? This study evaluates the **robustness** of **BERT** and a **BiLSTM** model on **Twitter sentiment analysis**, using datasets from **SemEval 2015 & 2017**. We apply **linguistic noise**, test both models under clean and perturbed conditions, and provide a comparative performance report.

...

## 📚 Citation

```text
Ibok, B. (2025). Robustness of Transformer-Based Models Against Linguistic Noise and Adversarial Inputs in Social Media Sentiment Tasks. Coventry University.
```